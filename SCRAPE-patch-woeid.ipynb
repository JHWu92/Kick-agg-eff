{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from src.url_helper import *\n",
    "from src.kick_parser import parse_commnunity_page\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime as dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def urls_to_srape_zip(zip_file):\n",
    "    df_list = []\n",
    "    with zipfile.ZipFile(zip_file, 'r') as f:\n",
    "        for fn in f.namelist():\n",
    "            df = pd.read_csv(f.open(fn))\n",
    "            df = df[(df.state=='failed') | (df.state=='successful')]\n",
    "            df['proj'] = df.urls.apply(lambda x: url_without_query_string(json.loads(x)['web']['project']))\n",
    "            # df['update'] = df.proj.apply(lambda x: x+'/updates')\n",
    "            # community page\n",
    "            df['community'] = df.proj.apply(lambda x: x+'/community')\n",
    "            df['proj_woeid'] = df.location.apply(lambda x: parse_url_qs(json.loads(x)['urls']['api']['nearby_projects'])['woe_id'][0] if not pd.isnull(x) else '')\n",
    "            \n",
    "            keep_cols = ['id', 'slug', 'community', 'proj_woeid']\n",
    "            df_keep = df[keep_cols]\n",
    "            df_list.append(df_keep)\n",
    "            \n",
    "    return pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scrape_woeid(projs_urls, headers_list, data_dir='data/scraped/patch-woeid/', save_step=10000, batch_cnt=0, debug=False, stop_batch=999999):\n",
    "    \"\"\"\n",
    "    batch_cnt=0: start from No. batch_cnt, which means start from idx=batch_cnt*save_step. \n",
    "                           E.g. batch_cnt = 30, save_step=500 meaning it will start from idx=15000\n",
    "    stop_batch=999999: stop before No. stop_batch, which means it stop when idx = stop_cnt*save_step \n",
    "                           (not including idx=stop_cnt*save step). \n",
    "                           E.g. stop_batch = 31, save step = 500, it will stop at idx=15500(the max idx scraped = 15499\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    batch_proj_ids = []\n",
    "    batch_res_cities = []\n",
    "    batch_res_countries = []\n",
    "    error_file_name = data_dir + 'url_req_errors-{}_{}-{}.csv'\n",
    "    batch_file_name_cities = data_dir + 'projs-back-cities-{}_{}-{}.csv'\n",
    "    batch_file_name_countries = data_dir + 'projs-back-countries-{}_{}-{}.csv'\n",
    "\n",
    "            \n",
    "    def save_batch(batch_cnt, len_batch):\n",
    "        # save parsed data in memory to disk\n",
    "        start = batch_cnt * save_step\n",
    "        end = start + len_batch - 1\n",
    "        # save and clear errors\n",
    "        df_errors = pd.DataFrame(errors, columns=['id','url', 'error'])\n",
    "        df_errors.to_csv(error_file_name.format(start, end, batch_cnt), encoding='utf-8')\n",
    "        del errors[:]\n",
    "        # save and clear scraped result\n",
    "        df_res = pd.DataFrame.from_dict(batch_res_cities)\n",
    "        df_res.set_index('id').to_csv(batch_file_name_cities.format(start, end, batch_cnt), encoding='utf-8')\n",
    "        del batch_res_cities[:]\n",
    "\n",
    "        df_res = pd.DataFrame.from_dict(batch_res_countries)\n",
    "        df_res.set_index('id').to_csv(batch_file_name_countries.format(start, end, batch_cnt), encoding='utf-8')\n",
    "        del batch_res_countries[:]\n",
    "        del batch_proj_ids[:]\n",
    "\n",
    "    for idx, row in projs_urls.iterrows():\n",
    "        # skip the first N batch\n",
    "        if idx<batch_cnt*save_step:\n",
    "            if idx==batch_cnt*save_step-1:\n",
    "                print 'skip idx = %d and everything before it' % idx, dtm.now()\n",
    "            continue\n",
    "        # stop at some batch\n",
    "        if idx>=stop_batch*save_step:\n",
    "            print 'stop scraping before idx =', idx\n",
    "            break\n",
    "        proj_id = row['id']\n",
    "        batch_proj_ids.append(proj_id)\n",
    "        url = row.community\n",
    "\n",
    "        html, error = req_html(proj_id, url, headers_list)\n",
    "        if html:\n",
    "            soup = BeautifulSoup(html)\n",
    "            res = {}\n",
    "            parse_commnunity_page(soup, res)\n",
    "            for c in json.loads(res['backer_cities']):\n",
    "                c['id'] = proj_id\n",
    "                batch_res_cities.append(c)\n",
    "            for c in json.loads(res['backer_countries']):\n",
    "                c['id'] = proj_id\n",
    "                batch_res_countries.append(c)\n",
    "        else:\n",
    "            errors.append(error)\n",
    "            \n",
    "        if (idx+1) % save_step == 0:\n",
    "            print 'save %dth batch' % batch_cnt, 'each batch: %d rows' % save_step, dtm.now()\n",
    "            save_batch(batch_cnt, len(batch_proj_ids))\n",
    "            batch_cnt += 1\n",
    "            \n",
    "    # if there are scraped pages left\n",
    "    if batch_proj_ids:\n",
    "        print 'save last batch of %d rows' % len(batch_proj_ids), dtm.now()\n",
    "        save_batch(batch_cnt, len(batch_proj_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'prepare urls', dtm.now()\n",
    "urls_df = urls_to_srape_zip('data/scraped/Kickstarter_2017-03-15T22_20_55_874Z.zip')\n",
    "print 'saving woeid projects', dtm.now()\n",
    "urls_df[['id', 'proj_woeid']].set_index('id').to_csv('data/projects_woeid.csv')\n",
    "\n",
    "headers_list = get_headers_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'scraping'\n",
    "scrape_woeid(urls_df, headers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cities_res = []\n",
    "# countries_res = []\n",
    "# errors = []\n",
    "# for i, row in urls_df.iterrows():\n",
    "#     url = row.community\n",
    "#     proj_id = row['id']\n",
    "#     html, error = req_html(proj_id, url, headers_list)\n",
    "#     if html:\n",
    "#         soup = BeautifulSoup(html)\n",
    "#         res = {}\n",
    "#         parse_commnunity_page(soup, res)\n",
    "#         for c in json.loads(res['backer_cities']):\n",
    "#             c['id'] = proj_id\n",
    "#             cities_res.append(c)\n",
    "#         for c in json.loads(res['backer_countries']):\n",
    "#             c['id'] = proj_id\n",
    "#             countries_res.append(c)\n",
    "#     else:\n",
    "#         errors.append(error)\n",
    "#     if (i+1) % 1500 == 0:\n",
    "#         print dtm.now(), 'crawled %d' %(i+1)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(countries_res).set_index('id').to_csv('data/projects_backers_countries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(cities_res).set_index('id').to_csv('data/projects_backers_cities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_errors = pd.DataFrame(errors, columns=['id','url', 'error'])\n",
    "# df_errors.to_csv('data/scraped/patch_woeid_errors.csv',encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
